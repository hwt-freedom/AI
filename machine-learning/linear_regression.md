# 线性回归
### 相关概念
* 线性回归属于 **监督学习** 的相关内容。
### 单变量线性回归算法模型表示
#### 变量定义
* $m$ 代表训练集中实例的数量（样本个数）
* $x$ 代表特征\输入变量
* $y$ 代表目标变量\输出变量
* $(x,y)$ 代表训练集中的实例
* $((x^i,y^i)$ 代表第 $i$ 个观察实例
* $h$ 代表学习算法的解决方案\函数，通常被称为假设函数(hypothesis)
#### 模型定义
$$h_\theta(x)=\theta_0+\theta_1x$$
* $h_\theta(x)$ 为假设函数
* $\theta_0,\theta_1$ 为模型参数
* 在算法训练过程中，期望得到最优的 $\theta_0,\theta_1$ 的值使得模型最为匹配训练集的数据
#### 代价函数
* 确定模型后，需要做的是选择合适的参数 $\theta_0,\theta_1$ ，使得模型所预测的值与训练集中的实际值之间的误差最小
* 损失函数(Loss Function)：
> 将预测值与实际值的平方差或者它们平方差的一半定义为损失函数，用来衡量预测输出值与实际值有多接近。

$$L(h(x),y)=\frac{1}{2}(h(x)-y)^2$$
* 代价函数(Cost Function)
> 将所有样本的损失值求和，可以得到一个代价函数

$$J(\theta_1,\theta_0) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^i)-y^i)^2$$
* 优化目标是选择合适的模型参数 $\theta_0,\theta_1$ 使得代价函数取得最小值
* 注意：在不同的模型中，**损失函数** 和 **代价函数** 可能是不同的，如分类问题的 **损失函数** 就与当前研究的线性回归算法的 **损失函数** 和 **代价函数** 不同。
<br>
<br>
### 算法训练
* 通过手工画图计算，能够大致地观察到代价函数在何处取到最小值，但要使计算机自动地求解出代价函数 $J(\theta_0,\theta_1)$ 的最小值，需要采用有效的算法求解
* 在计算机中，**梯度下降算法** 是一个用来求函数最小值的常用算法
#### 梯度下降算法（Gradient Descent）
##### 思想：
* 开始时随机选择一个参数的组合($\theta_0,\theta_1,...,\theta_n$)，计算代价函数，然后寻找一个能让代价函数值下降的最多的参数组合，持续做直到得到一个 **局部最小值** (local minimum)
* 选择不同的初始参数组合，可能会找到不同的局部最小值，当尝试完所有的参数组合后，即可得到 **全局最小值** (global minimum)
##### 算法过程
* 不断更新 $\theta_0,\theta_1$ 参数，使得代价函数 $J(\theta_1,\theta_0)$ 收敛到一个值
##### 相关名词解释
* $\alpha$ 学习率(learning rate)：学习率决定了沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，它只会影响算法收敛速度的快慢，因此选择合适的学习率很重要。
* 梯度(Gradient)：偏导数 $\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1)$ 即为点 $(\theta_0,\theta_1)$ 处的梯度值
* 同步更新(Simultaneous update)：梯度下降算法中，更新 $\theta_0,\theta_1$ 时需要同步更新两者，同步更新是一种更加自然的实现方法，在向量化实现中更加容易。

#### 参考博客
* [机器学习(一)——单变量线性回归](https://blog.csdn.net/lijiecao0226/article/details/78090453?utm_source=blogxgwz9)

### 多变量线性回归算法
