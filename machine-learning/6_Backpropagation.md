## 反向传播算法
### 神经网络的代价函数
#### 变量标记定义
* $L$ 表示神经网络的层数
* $S_l$ 表示第 $l$ 层中神经元的数目（不包括偏差单元）
* $K$ 表示输出单元/类的数目

#### 二元分类与多类别分类
<div align="center">
<img src="https://raw.githubusercontent.com/hwt-freedom/AI/master/machine-learning/picture/6_Backpropagation/Nerual_Network4.png" width = "400">
</div>

#### 逻辑回归的代价函数
* 对于逻辑回归，输出变量个数为1
$$J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}[y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2$$

#### 神经网络的代价函数
* 对于神经网络，输出变量的个数为$K$，用 $h_{\Theta}(x)_k$ 表示第 $k$ 个输出的假设，此时神经网络的代价函数将会是逻辑回归的一个综合泛化
$$J(\Theta) = -\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}[y_k^ilog(h_\theta(x^i)_k)+(1-y_k^i)log(1-h_\theta(x^i)_k)]+\frac{\lambda}{2m}\sum_l^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{j,i}^l)^2$$
> 代价函数的第一部分：在逻辑回归的基础上，对所有$K$个输出节点求和\
> 代价函数的第二部分：对神经网络所有权值矩阵的参数做正则化处理，遍历层数、行、列，且不包括bias单元

### 误差反向传播
#### 误差反向传播的目的
* 与线性回归和逻辑回归类似，也需要找到使神经网络代价函数 $J(\Theta)$ 最小时对应的 $\theta$ 参数
* 若运用梯度下降法求解 $min_{\Theta}J(\Theta)$ ，则关键步骤在于获得 $J(\Theta)$ 对神经网络各层矩阵参数的偏导值 $\frac{\partial}{\partial\Theta_{i,j}^l}J(\theta)$
* 误差反向传播的目的即为更快地计算偏导值

#### 反向传播算法的具体过程
##### part1：利用前向传播算法计算神经网络每一层的激活单元
* 设$a^1:=x^t$
* 执行前向传播以计算 $a^l$，$l=2,3,...,L$

##### part2：利用最后一层输出的预测结果与训练集的误差，运用反向传播算法计算每一层的误差
* 2.1 为训练集构造 $\Delta$ 误差矩阵，作为“累加器”将所有样本的误差累加，其中 $\Delta_{ij}^l$ 表示第$l$层的第 $i$ 个激活单元，受到下一层第 $j$ 个参数影响而导致的误差
* 2.2 对每个样本计算 $\delta^{L-1},\delta^{L-2},...,\delta^{2}$
* 2.3 计算完毕之后将每个样本的误差求和
 $$\Delta_{ij}^l:=\Delta_{ij}^l+a_j^l\delta_i^{l+1}$$
* 2.4  正则化处理
$$D_{ij}^l:=\frac{1}{m}(\Delta_{ij}^l+\lambda\Theta_{ij}^l) \quad if \; j\neq0$$
$$D_{ij}^l:=\frac{1}{m}\Delta_{ij}^l \quad if \; j=0$$

#### 反向传播算法的进一步理解
* 在二元分类且忽略正则化的情况下，若只考虑单样本，则cost函数如下：
$$cost(t)=y^tlog(h_{\Theta}(x^t))+(1-y^t)log(1-h_{\Theta}(x^t))$$
* 则对于第 $l$ 层的第 $j$ 个神经元，其 $\delta$ 值实际上是cost函数的导数
$$\delta_j^l=\frac{\partial}{\partial z_j^l}cost(t)$$

* [知乎-如何直观地解释 backpropagation 算法](https://www.zhihu.com/question/27239198/answer/89853077)

### 实际运用神经网络的技巧
#### 参数展开
* 具体含义：由于神经网络包含较多的权值矩阵，因此在实际运用时，可能需要将所有矩阵“展开”，并将它们放入一个长向量中，处理完毕后，再利用相关语法从长向量中取出，reshape为矩阵。
> 矩阵表示：正向传播和反向传播时，矩阵表示更加方便计算\
> 向量表示：部分高级优化算法要求使用长向量的形式进行计算

#### 梯度检验
* 梯度检验的目的：当神经网络模型较为复杂时，使用反向传播算法时可能会出现难以觉察的bug，因此需要进行梯度检测，确保反向传播算法是正确的。
> 实质：两者都能近似表示梯度信息，不同之处在于反向传播算法计算梯度时的运算速度要显著高于数值检验时的运算速度。

* 代价函数在某个神经元处偏导数近似计算公式
$$\frac{\partial}{\partial\Theta_j}J(\Theta)\approx\frac{J(\Theta1,...,\Theta_j+\epsilon,...,\Theta_n)-J(\Theta1,...,\Theta_j-\epsilon,...,\Theta_n)}{2\epsilon}$$
> $\epsilon$ 一般情况下取值为 $10^{-4}$

* 由于近似计算公式运算速度较慢，一旦验证了反向传播算法是正确的，就不需要再次进行梯度估算

#### 随机初始化参数
* 无论是梯度下降算法还是高级优化算法，都需要给 $\Theta$ 赋初值
* 对于神经网络而言，在赋初值时，若令所有参数同为0或者任意其他值，会使第二层所有的激活单元都有相同的值，因此在初始化时，需要对 $\Theta$ 进行随机初始化，使其在 $[-\epsilon,\epsilon]$ 内取值，以打破平衡对称。

### 神经网络训练总结
#### 选择神经网络架构（神经元之间不同的连接方式）
* 输入层神经元的个数：特征$x^i$的维度
* 使每个隐藏层的神经元个数相同，一般而言，隐藏单元越多越好
* 输出层神经元的个数：类别的数目
> 一般需要确定隐藏层的层数和中间层的神经元数目
#### 训练神经网络的步骤
1、随机初始化权重\
2、对任意的样本$x^i$实现和运用前向传播算法得到 $h_{\Theta}(x^i)$\
3、计算代价函数 $J(\Theta)$ 的值\
4、利用反向传播算法计算偏导数 $\frac{\partial}{\partial \Theta_{jk}^l}J(\Theta)$\
5、使用梯度检查方法比较偏导数值与估计值之间的关系\
6、使用梯度下降算法或高级优化算法与反向传播算法相结合使得 $J(\Theta)$ 最小
