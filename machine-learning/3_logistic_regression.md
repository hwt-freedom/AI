# 逻辑回归
### 分类问题(classfication)
#### 相关案例
* 判断邮件是否为垃圾邮件
* 判断网上交易是否为欺骗性交易
* 判断肿瘤是恶性还是良性
#### 显著特征
* 对于所有案例，分类的结果是离散值

### 假设表示(Hypothesis Representation)—分类的模型表示
#### 线性回归解决分类问题的局限性
* 对于分类问题，线性回归得到的拟合曲线通常会随着训练集的变化而产生较大的波动
* 假设函数存在函数值显著大于1和小于0的情况，这在分类问题中没有意义
* 需要构造特殊函数使得
$$0\leq h_\theta(x) \leq 1$$

### 逻辑回归模型
#### 模型定义
$$h_\theta(x)=g(\theta^TX)$$
* $X$ 代表特征向量
* $g$ 代表逻辑函数（logistic function），一个常用的逻辑函数为S形函数(Sigmoid function)
* Sigmoid 函数为 $g(z)=\frac{1}{1+e^{-z}}$

#### 决策边界(Desicion boundary)
* 对于Sigmoid 函数$g(z)=\frac{1}{1+e^{-z}}$
>当 $z>0$时，$g(z)>0.5$\
>当 $z<0$时，$g(z)<0.5$

* 对于假设函数 $h_\theta(x)=g(\theta^TX)$
>$\theta^TX \geq 0$ 时，$h_\theta(x)\geq0.5$，预测 $y=1$\
>$\theta^TX < 0$ 时，$h_\theta(x)<0.5$，预测 $y=0$

* 决策边界是假设函数的一个性质，由参数 $\theta$ 决定，不是数据集的属性
> 训练集→（拟合）→参数 $\theta$ →（决定）→ 决策边界

#### 代价函数
* 对于线性回归，代价函数为
$$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}(h_\theta(x^i)-y^i)^2$$
* 定义 $Cost(h_\theta(x^i),y^i)$ 为
$$Cost(h_\theta(x^i),y^i)=\frac{1}{2}(h_\theta(x^i)-y^i)^2$$
> $Cost(h_\theta(x^i),y^i)$ 的含义：对于某个样本，当输出预测值为 $h_\theta(x)$ 而实际标签为 $y$ 的情况下，我们希望学习算法付出的代价

* 对于逻辑回归，若继续使用线性回归中的Cost函数，即将sigmoid函数代入时，由于sigmoid函数的非线性性质，得到的代价函数将会是关于 $\theta$ 的非凸函数，在使用梯度下降算法时难以保证代价函数收敛到全局最小。
* 为使逻辑回归的Cost函数是凸函数，定义逻辑回归的Cost函数为
$$Cost(h_\theta(x^i),y^i)=\begin{cases}
&-log(h_\theta(x^i)) &if \quad y^i  = 1\\
&-log(1-h_\theta(x^i))  &if \quad y^i = 0
\end{cases}$$
> 既保证了sigmoid函数代入Cost函数时为凸函数，也使得在y=1时预测值为1的代价为0，预测值为0的代价为无穷大

* 进一步地，对逻辑回归的Cost函数进行合并简化
$$Cost(h_\theta(x^i),y^i)=-y^ilog(h_\theta(x^i))-(1-y^i)log(1-h_\theta(x^i))
$$

* 最终可以得到逻辑回归的代价函数
$$J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}[y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))]$$

#### 逻辑回归的梯度下降算法
* 不断更新 $\theta$ 值使得代价函数最小
$$\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^i)-y^i)x_j^i$$
> 上式的形式与线性回归相同，不同之处在于 $h_\theta(x^i)$ 是不相同的

#### 高级优化算法
* 在求解使代价函数最小时，除梯度下降之外，还有更为复杂和优越的高级算法，具体有共轭梯度(Conjugate Gradient)，局部优化法(BFGS)，有限内存局部优化法(LBFGS)。
> 高级优化算法不需要选择学习率，计算速度也显著高于梯度下降算法\
> 高级算法的缺点在于计算量很大

* 通常在使用高级优化算法时，不需要关注算法的具体实现细节
* 当机器学习的问题规模非常大时，具有数目庞大的特征量时，可以考虑选择使用高级优化算法加速模型的训练

#### 多类别分类
* 实现思想：将K分类问题转化为K个二分类问题
* 示例：假如要对具有3个类型的数据集进行分类，可以转化为3个二分类问题，将一个类标记为正向类，再将其他所有类都标记为负向类，模型记为 $h_\theta^i(x)$
> 对新的 $x$ 值进行预测时，在所有 $h_\theta^i(x)$ 中选择最大的，即可信度效果最好的类别。
