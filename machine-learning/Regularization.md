## 正则化
### 过拟合相关概念
#### 过拟合和欠拟合的概念
* 欠拟合：无法很好地适应训练集
* 过拟合：过于强调拟合原始数据，丢失了算法的本质
> 虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好

#### 如何解决过拟合的问题
* 尽量减少特征的数量
> 手工选择要保留的特征：把认为重要的特征保留\
> 使用模型选择算法：可以自动选择哪些特征保留，哪些特征舍弃

* 正则化
> 保留所有的特征，但是减少参数 $\theta_j$ 的大小\
> 此方法在特征数量很多时效果出色，并且每个特征都能为预测 $y$ 值作出贡献

### 修改代价函数实现正则化
* 对于回归问题，当高次项的参数 $\theta_j$ 较大时，容易产生过拟合问题，若能使得高次项的参数接近于0，则能够较好地拟合数据。
* 具体思想：将参数值减小，参数值减小意味着
> 更简单的假设模型\
> 更不容易出现过拟合的问题

* 由于难以在众多特征中挑选出相关度较低的参数，因此对所有特征的参数进行惩罚操作，修改后的代价函数如下
 $$J(\theta) = \frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^i)-y^i)^2+\lambda\sum_{j=1}^{n}\theta_j^2]$$
> 根据惯例，没有给 $\theta_0$ 增加惩罚项\
> 正则化后，在图像上表现为弯曲的部分被“捋平”，泛化能力增强

* $\lambda$ 表示正则化参数，控制如下两个目标之间的平衡
> 目标1：与代价函数的第一项有关，以便能够更好地拟合训练集\
> 目标2：与代价函数的第二项有关，使模型参数尽可能小，使模型相对简单

* $\lambda$ 值不允许过大或过小
> 当 $\lambda$ 值过大时，所有的参数都会接近于0，只剩 $\theta_0$ 一项，相当于只有一条直线拟合数据，产生欠拟合的问题\
> 当 $\lambda$ 值过小时，过拟合问题仍然无法解决

### 正则化线性回归
* 正则化线性回归的代价函数为
$$J(\theta) = \frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^i)-y^i)^2+\lambda\sum_{j=1}^{n}\theta_j^2]$$
#### 梯度下降方法的正则化处理
* 由于在代价函数中，未对 $\theta_0$ 进行正则化，因此首先将 $\theta_0$ 从其他参数中分离
$$\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^i)-y^i)x_0^i$$
$$\theta_j:=\theta_j-\alpha[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^i)-y^i)x_j^i+\frac{\lambda}{m}\theta_j]$$
* 通过整理，可以将除 $\theta_0$ 以外的公式调整为如下形式
$$\theta_j:=\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^i)-y^i)x_j^i$$
* 等式中的第一项 $1-\alpha\frac{\lambda}{m}$ 总是小于1，说明在每次更新时将 $\theta_j$ 的值减少一定量
#### 正规方程方法的正则化处理
* 常规的正规方程方法解决线性回归问题时，求 $\theta$ 的公式如下
$$\theta = (X^TX)^{-1}X^Ty$$
* 正则化处理后，求解公式转变为
$$\theta = (X^TX+\lambda L)^{-1}X^Ty$$
* 其中L矩阵的维度为 $(n+1) * (n+1)$
$$L = \begin{bmatrix}
0 & & & \\
& 1 & &\\
& & 1 &\\
& & &\ddots
\end{bmatrix}$$

* 对于常规的正规方程解法，当样本数 $m$ 小于特征数 $n$ 时，$X^T
X$ 是奇异矩阵，正则化处理后 $X^TX+\lambda L$ 必定不是奇异矩阵，必定可逆，因此正则化处理后还能解决正则化处理不可逆的问题

### 正则化逻辑回归
* 正则化逻辑回归的代价函数为
$$J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}[y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))]+ \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2$$

#### 梯度下降方法的正则化处理
* 与线性回归的梯度下降算法相同，也能得到如下的表达式，但由于线性回归和逻辑回归的假设函数不同，因此具体算法还是有较大差别
$$\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^i)-y^i)x_0^i$$
$$\theta_j:=\theta_j-\alpha[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^i)-y^i)x_j^i+\frac{\lambda}{m}\theta_j]$$

#### 高级算法的正则化处理
* 相对于正则化之前的算法，在设置代价函数 $J(\theta)$ 时以及在设置梯度时的相关参数有所变化。
